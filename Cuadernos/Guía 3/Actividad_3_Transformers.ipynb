{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad: Implementación de Transformers para Clasificación de Textos\n",
    "\n",
    "## Propósito de Aprendizaje\n",
    "Adquirir habilidades prácticas en el uso de modelos avanzados de Transformers, específicamente BERT, para la clasificación de textos. Al finalizar, los estudiantes habrán desarrollado la capacidad de implementar, entrenar y evaluar un modelo BERT, y compararlo con modelos anteriores.\n",
    "\n",
    "## Producto(s)\n",
    "- **Cuaderno Jupyter (Jupyter Notebook):** Documentar el proceso completo de preprocesamiento, implementación, entrenamiento y evaluación del modelo BERT.\n",
    "- **Informe (PDF/Markdown):** Un documento que resuma los hallazgos, incluyendo gráficos y análisis de las métricas de rendimiento.\n",
    "\n",
    "\n",
    "## Ejercicio 1: Carga y Preprocesamiento de Datos\n",
    "\n",
    "**Objetivo:** Familiarizarse con el conjunto de datos y prepararlos para el entrenamiento del modelo BERT.\n",
    "\n",
    "**Acciones:**\n",
    "1. Cargar el archivo `Noticias.xlsx` que contiene las noticias.\n",
    "2. Preprocesar los datos, incluyendo tokenización y padding de las secuencias de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "# Importar bibliotecas necesarias\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "file_path = '../../Datos/Datos Crudos/Noticias.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Eliminar las filas con valores faltantes\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Preprocesar los datos\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "data['input_ids'] = data['contenido'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "max_len = 128\n",
    "data['input_ids'] = data['input_ids'].apply(lambda x: x[:max_len] + [0]*(max_len-len(x)) if len(x) < max_len else x[:max_len])\n",
    "\n",
    "## Etiqueta para detectar deportes y filtrar noticias de archivo\n",
    "\n",
    "data=data[data['Etiqueta']!=\"archivo\"]\n",
    "\n",
    "data['Etiqueta'] = [1 if x == \"deportes\" else 0 for x in data[\"Etiqueta\"]]\n",
    "\n",
    "\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y validación\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['input_ids'].tolist(), data['Etiqueta'].tolist(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir los datos a tensores\n",
    "train_encodings = torch.tensor(X_train)\n",
    "val_encodings = torch.tensor(X_val)\n",
    "train_labels = torch.tensor(y_train.reshape(-1, 1))\n",
    "val_labels = torch.tensor(y_val.reshape(-1, 1))\n",
    "\n",
    "## attention masks\n",
    "\n",
    "train_masks = torch.tensor([[float(i != 0) for i in ii] for ii in X_train])\n",
    "val_masks = torch.tensor([[float(i != 0) for i in ii] for ii in X_val])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Implementación de BERT\n",
    "\n",
    "**Objetivo**: Construir y entrenar un modelo BERT para clasificar las noticias.\n",
    "\n",
    "**Acciones**:\n",
    "\n",
    "1. Utilizar la biblioteca transformers de Hugging Face para cargar el modelo BERT preentrenado.\n",
    "2. Ajustar el modelo con el conjunto de datos de entrenamiento.\n",
    "3. Evaluar el modelo utilizando un conjunto de datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "id2label = {0: \"no_deportes\", 1: \"deportes\"}\n",
    "label2id = {\"no_deportes\": 0, \"deportes\": 1}\n",
    "\n",
    "labels = list(id2label.keys())\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\",\n",
    "                                                           num_labels=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "metric_name = \"f1\"\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"input_ids\": train_encodings, \"attention_mask\": train_masks, \"labels\": train_labels})\n",
    "val_dataset = Dataset.from_dict({\"input_ids\": val_encodings, \"attention_mask\": val_masks, \"labels\": val_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Análisis de Resultados\n",
    "**Objetivo**: Evaluar y comparar el rendimiento del modelo BERT.\n",
    "\n",
    "**Acciones**:\n",
    "\n",
    "Calcular métricas de rendimiento como precisión, recall y F1-score.\n",
    "Visualizar las curvas de aprendizaje y los resultados de validación.\n",
    "Comparar los resultados con los modelos RNN y LSTM implementados anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluar el modelo\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "# Calcular métricas adicionales\n",
    "y_pred = trainer.predict(torch.utils.data.TensorDataset(val_encodings, val_labels)).predictions.argmax(axis=1)\n",
    "print(classification_report(y_val, y_pred, target_names=data['Etiqueta'].unique()))\n",
    "\n",
    "# Visualización de resultados\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(trainer.state.log_history)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Curvas de Aprendizaje - BERT')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4: Informe y Conclusiones\n",
    "\n",
    "**Objetivo**: Documentar los resultados obtenidos y discutir las implicaciones prácticas.\n",
    "\n",
    "**Acciones**:\n",
    "\n",
    "Documentar los resultados obtenidos en un informe detallado, incluyendo gráficos y análisis de las métricas de rendimiento.\n",
    "Discutir las implicaciones de los resultados para aplicaciones prácticas de NLP, como la automatización de la clasificación de noticias en sistemas de recomendación o análisis de medios.\n",
    "Proponer posibles mejoras y futuras direcciones de investigación, como la exploración de arquitecturas híbridas o la integración de mecanismos de atención."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_ean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
