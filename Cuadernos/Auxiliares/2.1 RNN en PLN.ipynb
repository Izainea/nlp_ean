{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Redes Neuronales Recurrentes (RNN) en Procesamiento de Lenguaje Natural (NLP)\n","\n","En este cuaderno, implementaremos y experimentaremos con redes neuronales recurrentes (RNN) y sus variantes, como las LSTM y GRU, aplicándolas a tareas comunes de procesamiento de lenguaje natural (NLP).\n","\n","## Requisitos previos\n","- Python 3.7+\n","- Numpy\n","- TensorFlow\n","\n","## Objetivo\n","1. Comprender la arquitectura y funcionamiento de las RNN y sus variantes.\n","2. Implementar RNN, LSTM y GRU aplicadas a tareas de clasificación de texto.\n","3. Evaluar y comparar el rendimiento de cada modelo."]},{"cell_type":"markdown","metadata":{},"source":["## 1. Cargando y Preprocesando el Dataset\n","En esta sección, cargaremos un dataset de texto para realizar clasificación. Utilizaremos técnicas de preprocesamiento como la tokenización y el padding para preparar los datos para los modelos.\n"]},{"cell_type":"code","execution_count":4,"id":"c0788705","metadata":{},"outputs":[],"source":["### Leemos el documento en txt \n","\n","import requests\n","import re\n","import os\n","\n","url = \"https://gist.githubusercontent.com/ismaproco/6781d297ee65c6a707cd3c901e87ec56/raw/20d3520cd7c53d99215845375b1dca16ac827bd7/gabriel_garcia_marquez_cien_annos_soledad.txt\"\n","\n","response = requests.get(url)\n","\n","if response.status_code == 200:\n","    with open(\"gabriel_garcia_marquez_cien_annos_soledad.txt\", \"w\") as file:\n","        file.write(response.text)\n","else:\n","    print(\"Error al descargar el archivo\")\n","\n","# Leemos el archivo\n","\n","with open(\"gabriel_garcia_marquez_cien_annos_soledad.txt\", \"r\") as file:\n","    texto = file.read()\n","\n","# Arreglamos el texto\n","\n","\n","texto = texto.replace(\"\\n\", \" \")\n","texto = texto.replace(\"\\r\", \" \")\n","texto = texto.replace(\"Gabriel García Márquez\", \" \")\n","texto = texto.replace(\"Cien años de soledad\", \" \")\n","texto = texto.replace(\"  \", \" \")\n","\n","texto = re.sub(r'\\s+', ' ', texto)\n","\n","texto = texto.replace('EDITADO POR \"EDICIONES LA CUEVA\" Para J omi García Ascot y María Luisa Elio ', \" \")\n","\n","## Guardamos el texto arreglado\n","\n","with open(\"gabriel_garcia_marquez_cien_annos_soledad.txt\", \"w\") as file:\n","    file.write(texto)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import pandas as pd\n","\n","# Cargar el dataset\n","\n","data = pd.read_csv('path_to_dataset.csv')  # Reemplaza con el dataset adecuado\n","\n","# Preprocesamiento del texto\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(data['text_column'])\n","sequences = tokenizer.texts_to_sequences(data['text_column'])\n","\n","# Padding para tener secuencias de igual longitud\n","padded_sequences = pad_sequences(sequences, padding='post', maxlen=100)\n","padded_sequences[:5]  # Muestra las primeras 5 secuencias procesadas"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Implementando una Red Neuronal Recurrente (RNN) Simple\n","Ahora implementaremos una RNN simple usando TensorFlow. Este modelo será nuestro punto de partida para comprender cómo funciona una RNN aplicada a NLP."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Crear una red RNN simple\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=5000, output_dim=64, input_length=100),\n","    tf.keras.layers.SimpleRNN(64),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compilar el modelo\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Entrenar el modelo\n","history = model.fit(padded_sequences, data['label_column'], epochs=5, validation_split=0.2)"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Implementando LSTM\n","A continuación, reemplazaremos la capa SimpleRNN con una capa LSTM y entrenaremos el modelo de nuevo. Esto nos permitirá comparar el rendimiento de ambos enfoques."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Crear una red LSTM\n","model_lstm = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=5000, output_dim=64, input_length=100),\n","    tf.keras.layers.LSTM(64),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compilar el modelo\n","model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Entrenar el modelo\n","history_lstm = model_lstm.fit(padded_sequences, data['label_column'], epochs=5, validation_split=0.2)"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Evaluación del Modelo\n","En esta sección, evaluaremos ambos modelos (RNN y LSTM) y visualizaremos su rendimiento durante el entrenamiento usando gráficos."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Visualizar la pérdida\n","plt.plot(history.history['loss'], label='RNN')\n","plt.plot(history_lstm.history['loss'], label='LSTM')\n","plt.title('Pérdida del modelo')\n","plt.xlabel('Épocas')\n","plt.ylabel('Pérdida')\n","plt.legend()\n","plt.show()\n","\n","# Evaluar ambos modelos\n","rnn_eval = model.evaluate(padded_sequences, data['label_column'])\n","lstm_eval = model_lstm.evaluate(padded_sequences, data['label_column'])\n","print(f\"Evaluación RNN: {rnn_eval}\")\n","print(f\"Evaluación LSTM: {lstm_eval}\")"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Implementando GRU\n","Finalmente, implementaremos una red GRU. Las GRU son una variante simplificada de las LSTM, y compararemos su rendimiento con las RNN y LSTM."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Crear una red GRU\n","model_gru = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=5000, output_dim=64, input_length=100),\n","    tf.keras.layers.GRU(64),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compilar el modelo\n","model_gru.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Entrenar el modelo\n","history_gru = model_gru.fit(padded_sequences, data['label_column'], epochs=5, validation_split=0.2)"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Conclusión y Reflexión\n","Reflexiona sobre las ventajas y desventajas de cada tipo de red recurrente. ¿Qué tipo de red fue más eficaz en este dataset? ¿Qué diferencias observaste entre la RNN, LSTM y GRU en términos de rendimiento y tiempo de entrenamiento?"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":5}
