{"cells":[{"cell_type":"markdown","id":"7d2ddd17","metadata":{},"source":["# Explicación de Redes Neuronales Recurrentes (RNN)\n","\n","En este cuaderno, explicaremos cómo funcionan las Redes Neuronales Recurrentes (RNN), sus aplicaciones, limitaciones y cómo se comparan con otras arquitecturas como LSTM y Transformers.\n","\n","## Objetivos\n","- Comprender el funcionamiento básico de una RNN.\n","- Explorar cómo las RNN procesan secuencias de datos, especialmente en tareas de texto.\n","- Discutir las limitaciones de las RNN y sus desafíos.\n","- Comparar las RNN con LSTM y Transformers."]},{"cell_type":"markdown","id":"1ae33e51","metadata":{},"source":["## 1. Introducción a las RNN\n","Las RNN son un tipo de red neuronal diseñada para trabajar con datos secuenciales, como texto, series temporales o secuencias de video. Su estructura les permite recordar información de estados anteriores, lo que las hace útiles para tareas en las que el contexto importa.\n","\n","### Funcionamiento básico de una RNN\n","En una RNN, la salida de una celda recurrente en un paso temporal es pasada como entrada a la celda en el siguiente paso. Esto permite que la red acumule información a medida que avanza por la secuencia. \n","\n","![Arquitectura básica de una RNN](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/2880px-Recurrent_neural_network_unfold.svg.png)\n","\n","En la imagen, podemos ver cómo la misma celda se repite para cada paso en la secuencia, compartiendo los mismos pesos y acumulando el contexto a lo largo del tiempo."]},{"cell_type":"markdown","id":"ccc74386","metadata":{},"source":["## 2. Procesamiento de Secuencias en una RNN\n","Veamos un ejemplo de cómo una RNN procesa una secuencia de texto para predecir la siguiente palabra en la secuencia. Usaremos un conjunto simple de oraciones.\n"]},{"cell_type":"code","execution_count":null,"id":"f5b21f46","metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n","import numpy as np\n","\n","# Datos de ejemplo\n","sentences = [\"el gato juega\", \"el perro corre\", \"la luna brilla\"]\n","tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","sequences = tokenizer.texts_to_sequences(sentences)\n","max_sequence_len = max([len(x) for x in sequences])\n","sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_sequence_len)\n","\n","# Dividir en características (X) y etiquetas (y)\n","X, y = sequences[:, :-1], sequences[:, -1]\n","y = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index)+1)\n","\n","# Crear el modelo RNN\n","model_rnn = Sequential([\n","    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_sequence_len-1),\n","    SimpleRNN(32),\n","    Dense(len(tokenizer.word_index)+1, activation='softmax')\n","])\n","\n","# Compilar el modelo\n","model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model_rnn.summary()"]},{"cell_type":"markdown","id":"66dcd870","metadata":{},"source":["### Entrenamiento de la RNN\n","Entrenamos el modelo con las secuencias anteriores."]},{"cell_type":"code","execution_count":null,"id":"1360da05","metadata":{},"outputs":[],"source":["# Entrenar el modelo\n","history_rnn = model_rnn.fit(X, y, epochs=200, verbose=1)\n"]},{"cell_type":"markdown","id":"204433f2","metadata":{},"source":["### Visualización del rendimiento\n","Mostramos las curvas de precisión y pérdida del modelo durante el entrenamiento."]},{"cell_type":"code","execution_count":null,"id":"7aa3f9e6","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","# Graficar la precisión y la pérdida\n","plt.plot(history_rnn.history['accuracy'], label='Precisión')\n","plt.plot(history_rnn.history['loss'], label='Pérdida')\n","plt.xlabel('Épocas')\n","plt.ylabel('Valor')\n","plt.title('Rendimiento del modelo RNN')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","id":"349c9072","metadata":{},"source":["## 3. Limitaciones de las RNN\n","Las RNN enfrentan varios desafíos, entre ellos:\n","- **Desvanecimiento del gradiente**: A medida que la secuencia se hace más larga, las RNN tienen dificultades para recordar información de los primeros pasos, lo que limita su capacidad para manejar dependencias a largo plazo.\n","- **Paralelización**: Debido a la naturaleza secuencial de las RNN, es difícil paralelizar su entrenamiento, lo que las hace más lentas en comparación con otros modelos más recientes.\n"]},{"cell_type":"markdown","id":"ecf17697","metadata":{},"source":["### Ejemplo de desvanecimiento de gradiente\n","Probemos con secuencias más largas para ver cómo afecta la precisión de las predicciones."]},{"cell_type":"code","execution_count":null,"id":"1f025e8a","metadata":{},"outputs":[],"source":["# Secuencias más largas\n","long_sentences = [\"el gato juega en el jardín mientras el sol brilla\", \"la luna llena ilumina el cielo nocturno\"]\n","long_sequences = tokenizer.texts_to_sequences(long_sentences)\n","long_sequences = tf.keras.preprocessing.sequence.pad_sequences(long_sequences, maxlen=max_sequence_len)\n","\n","# Predicciones del modelo\n","predictions = model_rnn.predict(long_sequences)\n","predicted_words = [np.argmax(pred) for pred in predictions]\n","predicted_words_text = [list(tokenizer.word_index.keys())[list(tokenizer.word_index.values()).index(idx)] for idx in predicted_words]\n","\n","print(f\"Secuencias de entrada: {long_sentences}\")\n","print(f\"Palabras predichas: {predicted_words_text}\")"]},{"cell_type":"markdown","id":"9f20ffa2","metadata":{},"source":["## 4. Comparación con LSTM y Transformers\n","Las LSTM y GRU son variantes de las RNN que abordan algunos de estos problemas, como el desvanecimiento del gradiente. Los Transformers, por otro lado, eliminan completamente la dependencia secuencial, lo que permite un procesamiento paralelo y mayor eficiencia.\n","\n","### LSTM\n","Las LSTM utilizan celdas de memoria y puertas de entrada, olvido y salida para decidir qué información mantener o desechar. Esto les permite manejar dependencias a largo plazo de manera más efectiva.\n","\n","### Transformers\n","Los Transformers utilizan mecanismos de atención que permiten a cada posición en una secuencia de entrada relacionarse directamente con cualquier otra posición, eliminando la necesidad de procesar los datos en orden secuencial.\n","\n","En un cuaderno futuro, implementaremos estas variantes para comparar su rendimiento con las RNN."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.5"}},"nbformat":4,"nbformat_minor":5}
