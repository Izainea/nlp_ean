{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Word2Vec es una técnica que se utiliza para obtener representaciones vectoriales de palabras. Estas representaciones son útiles para realizar tareas de procesamiento de lenguaje natural, como la clasificación de texto, la traducción automática, la generación de texto, etc. Su objetivo es capturar las relaciones semánticas y sintácticas entre palabras basándose en su contexto dentro de un corpus de texto. Fue introducido por Mikolov et al. en 2013.\n",
    "\n",
    "Esta técnica se basa en la hipótesis distribucional, que establece que las palabras que aparecen en contextos similares tienen significados similares. Por lo tanto, Word2Vec se entrena para predecir las palabras vecinas de una palabra dada, utilizando un corpus de texto como datos de entrenamiento.\n",
    "\n",
    "Tiene dos variantes principales: Skip-gram y Continuous Bag of Words (CBOW). La diferencia entre ambas radica en la forma en que se plantea el problema de predicción. En el caso de Skip-gram, se predice el contexto a partir de una palabra dada, mientras que en CBOW se predice la palabra a partir de su contexto.\n",
    "\n",
    "Por ejemplo en el texto \"El gato come pescado\", si utilizamos una ventana de tamaño 2, el contexto de la palabra \"come\" sería \"El gato pescado\". En el caso de Skip-gram, se trataría de predecir \"El\", \"gato\", \"pescado\" a partir de \"come\":\n",
    "\n",
    "| Entrada | Salida |\n",
    "|---------|--------|\n",
    "| come    | El     |\n",
    "| come    | gato   |\n",
    "| come    | pescado|\n",
    "\n",
    "Mientras que en el caso de CBOW, se trataría de predecir \"come\" a partir de \"El\", \"gato\", \"pescado\":\n",
    "\n",
    "| Entrada | Salida |\n",
    "|---------|--------|\n",
    "| El      | come   |\n",
    "| gato    | come   |\n",
    "| pescado | come   |\n",
    "\n",
    "En este notebook, vamos a utilizar la implementación de Word2Vec de la librería Gensim para entrenar un modelo de Skip-gram y explorar las representaciones vectoriales de palabras obtenidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **representación inicial de palabras** en Word2Vec es una representación **discreta y esparsa** conocida como **one-hot encoding**. Es el punto de partida antes de que las palabras sean transformadas en vectores densos y continuos mediante el aprendizaje en Word2Vec. Vamos a desglosarlo con ejemplos para clarificar cómo funciona.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **One-Hot Encoding: Representación Básica**\n",
    "Dado un vocabulario de $ V $ palabras, asignamos un índice único a cada palabra. Luego, representamos cada palabra como un vector de dimensión $ V $, donde todas las posiciones son 0 excepto una, que es 1.\n",
    "\n",
    "#### Ejemplo Simple\n",
    "Supongamos un vocabulario con las siguientes palabras:\n",
    "$$\n",
    "\\text{Vocabulario} = \\{\\text{gato, perro, ratón, queso}\\}\n",
    "$$\n",
    "\n",
    "Asignamos índices:\n",
    "- $ \\text{gato} \\to 0 $\n",
    "- $ \\text{perro} \\to 1 $\n",
    "- $ \\text{ratón} \\to 2 $\n",
    "- $ \\text{queso} \\to 3 $\n",
    "\n",
    "La representación **one-hot** para cada palabra será:\n",
    "$$\n",
    "\\text{gato} = [1, 0, 0, 0], \\quad \\text{perro} = [0, 1, 0, 0], \\quad \\text{ratón} = [0, 0, 1, 0], \\quad \\text{queso} = [0, 0, 0, 1]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Propiedades del One-Hot Encoding**\n",
    "1. **Alta Dimensionalidad**:\n",
    "   Si el vocabulario tiene $ V = 10,000 $ palabras, cada palabra estará representada por un vector de tamaño $ 10,000 $, con un único valor de 1.\n",
    "\n",
    "2. **Discretas y Esparzas**:\n",
    "   Solo un elemento del vector es 1, mientras que el resto son ceros. Esto significa que la representación es **esparsa** y ocupa mucho espacio de memoria.\n",
    "\n",
    "3. **Sin Captura de Relación Semántica**:\n",
    "   La representación one-hot no tiene en cuenta ninguna relación entre palabras. Por ejemplo:\n",
    "   - $ \\text{gato} = [1, 0, 0, 0] $\n",
    "   - $ \\text{perro} = [0, 1, 0, 0] $\n",
    "\n",
    "   Aunque $ \\text{gato} $ y $ \\text{perro} $ están semánticamente relacionados (ambos son animales), sus representaciones one-hot son **ortogonales** y no comparten ninguna similitud matemática.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Limitaciones del One-Hot Encoding**\n",
    "#### A. **Dimensión del Vector**\n",
    "A medida que el vocabulario crece, la dimensión $ V $ se vuelve enorme, lo que hace que esta representación sea ineficiente.\n",
    "\n",
    "#### B. **Falta de Información Semántica**\n",
    "La distancia entre vectores no refleja similitudes entre palabras. Por ejemplo:\n",
    "$$\n",
    "\\text{Distancia Euclidiana}(\\text{gato}, \\text{perro}) = \\text{Distancia Euclidiana}(\\text{gato}, \\text{queso}) = \\sqrt{2}\n",
    "$$\n",
    "Esto significa que $ \\text{gato} $ está tan \"cerca\" de $ \\text{perro} $ como de $ \\text{queso} $, lo cual no es intuitivo.\n",
    "\n",
    "#### C. **Escalabilidad en Modelos**\n",
    "Los modelos de aprendizaje automático necesitan procesar estos vectores de alta dimensionalidad, lo cual aumenta la complejidad computacional y limita el rendimiento.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Transición a Representaciones Densas**\n",
    "La motivación de Word2Vec es superar estas limitaciones al transformar las palabras en **vectores densos** y de menor dimensión ($ d $) que capturen relaciones semánticas. Por ejemplo:\n",
    "\n",
    "- Representación inicial (one-hot): $ \\text{gato} = [1, 0, 0, 0] $\n",
    "- Representación Word2Vec: $ \\text{gato} = [0.2, -0.4, 0.8, 0.1, -0.3] $\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Ejemplo de Representación Inicial en un Contexto Real**\n",
    "Imaginemos una oración: \n",
    "$$\n",
    "\\text{\"El gato persigue al ratón\"}\n",
    "$$\n",
    "\n",
    "1. Vocabulario:\n",
    "   $$\n",
    "   \\{\\text{el, gato, persigue, al, ratón}\\}\n",
    "   $$\n",
    "\n",
    "2. Índices de las palabras:\n",
    "   - $ \\text{el} \\to 0 $\n",
    "   - $ \\text{gato} \\to 1 $\n",
    "   - $ \\text{persigue} \\to 2 $\n",
    "   - $ \\text{al} \\to 3 $\n",
    "   - $ \\text{ratón} \\to 4 $\n",
    "\n",
    "3. Representaciones one-hot:\n",
    "   - $ \\text{el} = [1, 0, 0, 0, 0] $\n",
    "   - $ \\text{gato} = [0, 1, 0, 0, 0] $\n",
    "   - $ \\text{persigue} = [0, 0, 1, 0, 0] $\n",
    "   - $ \\text{al} = [0, 0, 0, 1, 0] $\n",
    "   - $ \\text{ratón} = [0, 0, 0, 0, 1] $\n",
    "\n",
    "Cuando esta oración pasa por un modelo como Word2Vec, estas representaciones iniciales se transforman en vectores densos que reflejan las relaciones semánticas entre palabras. Por ejemplo:\n",
    "\n",
    "$$\n",
    "\\text{gato} \\approx [0.3, -0.2, 0.8], \\quad \\text{ratón} \\approx [0.2, -0.1, 0.7]\n",
    "$$\n",
    "\n",
    "Estas nuevas representaciones permiten cálculos más avanzados, como medir la similitud semántica entre palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Librerias necesarias Word2Vec\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar el texto\n",
    "\n",
    "Para este ejercicio inicial usaremos un texto sencillo, el cual se cargará en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_1 = \"\"\"Hace varios años, en el pelotón de fusilamiento, el coronel Aureliano Buendía había de recordar aquella tarde remota en que su padre lo llevó a conocer el hielo. \"\"\"\n",
    "texto_2 = \"\"\"Hace tanto tiempo que no me acuerdo de nada, pero recuerdo que mi padre me llevó a conocer el hielo. \"\"\"\n",
    "texto_3 = \"\"\"Hace tiempo que ocurrió la era de hielo, ahorita que solo soy un  perezoso recuerdo aquellos días tan bellos con mis amigos, un mamut y un dientes de sable. \"\"\"\n",
    "\n",
    "corpus = [texto_1, texto_2, texto_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento\n",
    "\n",
    "El preprocesamiento es una etapa importante en el procesamiento de lenguaje natural. En esta etapa se eliminan las palabras que no aportan información, como los signos de puntuación, las palabras vacías, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Preprocesamiento de texto ####################\n",
    "def preprocesamiento(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r\"[\\W\\d_]+\", \" \", texto)\n",
    "    texto = texto.split()\n",
    "\n",
    "    return texto\n",
    "\n",
    "corpus_procesado = [preprocesamiento(texto) for texto in corpus]\n",
    "\n",
    "print(corpus_procesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Arquitectura de Word2Vec**\n",
    "\n",
    "## **CBOW (Continuous Bag of Words)**\n",
    "\n",
    "CBOW busca predecir una palabra objetivo $ w_t $ a partir de sus palabras de contexto. Este proceso transforma representaciones **esparsas** (vectores one-hot) en representaciones **densas** (embeddings) que capturan relaciones semánticas globales entre palabras.\n",
    "\n",
    "---\n",
    "\n",
    "### **Notación Preliminar**\n",
    "1. **Vocabulario**: $ V $, el tamaño total del vocabulario.\n",
    "2. **Dimensión del embedding**: $ d $, la dimensión del espacio donde proyectamos las palabras.\n",
    "3. **Matrices de embedding**:\n",
    "   - $ \\mathbf{W} \\in \\mathbb{R}^{V \\times d} $: Matriz de embeddings de entrada.\n",
    "   - $ \\mathbf{U} \\in \\mathbb{R}^{V \\times d} $: Matriz de embeddings de salida.\n",
    "4. **Vector one-hot**:\n",
    "   - Cada palabra $ w $ se representa como $ \\mathbf{x} \\in \\mathbb{R}^V $, donde $ x_i = 1 $ si $ i $ es el índice de $ w $, y $ x_j = 0 $ para $ j \\neq i $.\n",
    "\n",
    "---\n",
    "\n",
    "## **CBOW en Lenguaje Matricial**\n",
    "\n",
    "### **Forward Pass**\n",
    "\n",
    "#### **Paso 1: Entrada - Palabras de Contexto**\n",
    "\n",
    "Dada la frase:\n",
    "$$\n",
    "\\text{\"El gato persigue al ratón\"}\n",
    "$$\n",
    "\n",
    "Queremos predecir $ w_t = \\text{\"persigue\"} $, utilizando como contexto:\n",
    "$$\n",
    "\\{\\text{\"El\"}, \\text{\"gato\"}, \\text{\"al\"}, \\text{\"ratón\"}\\}.\n",
    "$$\n",
    "\n",
    "1. Asignamos índices al vocabulario:\n",
    "$$\n",
    "\\{\\text{\"El\"} \\to 0, \\text{\"gato\"} \\to 1, \\text{\"persigue\"} \\to 2, \\text{\"al\"} \\to 3, \\text{\"ratón\"} \\to 4\\}.\n",
    "$$\n",
    "\n",
    "2. Representamos las palabras de contexto como vectores one-hot:\n",
    "$$\n",
    "\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 \\\\  % \"El\"\n",
    "0 & 1 & 0 & 0 & 0 \\\\  % \"gato\"\n",
    "0 & 0 & 0 & 1 & 0 \\\\  % \"al\"\n",
    "0 & 0 & 0 & 0 & 1     % \"ratón\"\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{4 \\times 5}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Paso 2: Transformación - Embeddings densos**\n",
    "\n",
    "La matriz de embeddings $ \\mathbf{W} \\in \\mathbb{R}^{V \\times d} $ transforma las representaciones esparsas en vectores densos. La operación es:\n",
    "$$\n",
    "\\mathbf{E}_{\\text{contexto}} = \\mathbf{X} \\mathbf{W}.\n",
    "$$\n",
    "\n",
    "Supongamos que $ d = 2 $ y:\n",
    "$$\n",
    "\\mathbf{W} =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 \\\\  % \"El\"\n",
    "0.3 & 0.4 \\\\  % \"gato\"\n",
    "0.5 & 0.6 \\\\  % \"persigue\"\n",
    "0.7 & 0.8 \\\\  % \"al\"\n",
    "0.9 & 1.0     % \"ratón\"\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Entonces:\n",
    "$$\n",
    "\\mathbf{E}_{\\text{contexto}} =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 \\\\  % \"El\"\n",
    "0.3 & 0.4 \\\\  % \"gato\"\n",
    "0.7 & 0.8 \\\\  % \"al\"\n",
    "0.9 & 1.0     % \"ratón\"\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{4 \\times 2}.\n",
    "$$\n",
    "\n",
    "Cada fila representa un embedding denso que captura las características semánticas latentes de cada palabra.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Paso 3: Promedio de los embeddings**\n",
    "\n",
    "Para resumir el contexto, calculamos el promedio de los vectores densos:\n",
    "$$\n",
    "\\mathbf{v}_{\\text{contexto}} = \\frac{1}{2k} \\sum_{i=1}^{2k} \\mathbf{E}_{\\text{contexto}}[i, :].\n",
    "$$\n",
    "\n",
    "En este caso:\n",
    "$$\n",
    "\\mathbf{v}_{\\text{contexto}} = \\frac{1}{4} \\begin{bmatrix} 0.1+0.3+0.7+0.9 & 0.2+0.4+0.8+1.0 \\end{bmatrix} = \\begin{bmatrix} 0.5 & 0.6 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Paso 4: Proyección al espacio del vocabulario**\n",
    "\n",
    "Proyectamos $ \\mathbf{v}_{\\text{contexto}} $ al espacio del vocabulario utilizando la matriz de salida $ \\mathbf{U} \\in \\mathbb{R}^{V \\times d} $:\n",
    "$$\n",
    "\\mathbf{z} = \\mathbf{U} \\mathbf{v}_{\\text{contexto}}.\n",
    "$$\n",
    "\n",
    "Por ejemplo, si:\n",
    "$$\n",
    "\\mathbf{U} =\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.3 \\\\  % \"El\"\n",
    "0.4 & 0.5 \\\\  % \"gato\"\n",
    "0.6 & 0.7 \\\\  % \"persigue\"\n",
    "0.8 & 0.9 \\\\  % \"al\"\n",
    "1.0 & 1.1     % \"ratón\"\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "entonces:\n",
    "$$\n",
    "\\mathbf{z} =\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.3 \\\\ \n",
    "0.4 & 0.5 \\\\ \n",
    "0.6 & 0.7 \\\\ \n",
    "0.8 & 0.9 \\\\ \n",
    "1.0 & 1.1\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\ \n",
    "0.6\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.28 \\\\  % \"El\"\n",
    "0.49 \\\\  % \"gato\"\n",
    "0.70 \\\\  % \"persigue\"\n",
    "0.91 \\\\  % \"al\"\n",
    "1.12     % \"ratón\"\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Paso 5: Softmax**\n",
    "\n",
    "Finalmente, aplicamos softmax para convertir $ \\mathbf{z} $ en una distribución de probabilidad:\n",
    "$$\n",
    "P(w_t | \\text{contexto}) = \\frac{\\exp(z_i)}{\\sum_{j=1}^V \\exp(z_j)}.\n",
    "$$\n",
    "\n",
    "La palabra con la mayor probabilidad será la predicción $ \\hat{w}_t $, que debería ser \"persigue\".\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Por qué CBOW Encuentra Relaciones Densas**\n",
    "\n",
    "1. **De esparso a denso**:\n",
    "   Transformar $ \\mathbf{x} $ mediante $ \\mathbf{W} $ reduce la dimensionalidad (de $ V $ a $ d $) y captura relaciones semánticas latentes.\n",
    "\n",
    "2. **Relaciones semánticas globales**:\n",
    "   Los embeddings densos reflejan la coocurrencia de palabras en contextos similares. Palabras como \"gato\" y \"ratón\", que aparecen juntas, tendrán vectores cercanos.\n",
    "\n",
    "3. **Eficiencia computacional**:\n",
    "   Al calcular un promedio, se reduce el impacto de palabras menos informativas como artículos y preposiciones.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Impacto en Semántica y Sintaxis**\n",
    "\n",
    "1. **Semántica**:\n",
    "   - CBOW captura relaciones globales. Por ejemplo, \"gato\" y \"perro\" tendrán embeddings cercanos si aparecen en contextos similares.\n",
    "\n",
    "2. **Sintaxis**:\n",
    "   - CBOW ignora el orden de las palabras. Por ejemplo, \"El gato persigue al ratón\" y \"El ratón persigue al gato\" tendrán la misma representación promedio, lo que puede ser una limitación en tareas sensibles al orden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Skip-gram**\n",
    "\n",
    "En Skip-gram, la dirección del problema se invierte: en lugar de predecir la palabra objetivo $ w_t $ a partir del contexto, ahora utilizamos $ w_t $ para predecir cada palabra del contexto. Esto significa que cada palabra central está asociada a múltiples predicciones, una para cada palabra en su ventana de contexto.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. ¿Qué se optimiza en Skip-gram?**\n",
    "\n",
    "El objetivo del modelo Skip-gram es **maximizar la probabilidad condicional de las palabras del contexto** $ \\{w_{t-k}, \\ldots, w_{t-1}, w_{t+1}, \\ldots, w_{t+k}\\} $, dado la palabra central $ w_t $. \n",
    "\n",
    "La pérdida del modelo está definida como la log-verosimilitud negativa sobre todas las palabras del contexto:\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t=1}^T \\sum_{j=-k, j \\neq 0}^k \\log P(w_{t+j} | w_t),\n",
    "$$\n",
    "donde:\n",
    "- $ P(w_{t+j} | w_t) $ es la probabilidad predicha de que $ w_{t+j} $ sea una palabra del contexto, dado $ w_t $.\n",
    "- $ T $ es el número total de palabras en el corpus.\n",
    "- $ k $ es el tamaño de la ventana de contexto.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Probabilidad condicional en Skip-gram**\n",
    "\n",
    "La probabilidad condicional $ P(w_{t+j} | w_t) $ se calcula utilizando la función softmax, de manera similar a CBOW:\n",
    "$$\n",
    "P(w_{t+j} | w_t) = \\frac{\\exp(\\mathbf{u}_{w_{t+j}}^\\top \\mathbf{v}_{w_t})}{\\sum_{i=1}^V \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_{w_t})},\n",
    "$$\n",
    "donde:\n",
    "- $ \\mathbf{v}_{w_t} $ es el embedding de la palabra central $ w_t $, obtenido de la matriz $ \\mathbf{W} $.\n",
    "- $ \\mathbf{u}_{w_{t+j}} $ es el embedding de salida de la palabra del contexto $ w_{t+j} $, tomado de la matriz $ \\mathbf{U} $.\n",
    "- $ V $ es el tamaño del vocabulario.\n",
    "\n",
    "La optimización ajusta los parámetros de $ \\mathbf{W} $ y $ \\mathbf{U} $ para maximizar la probabilidad de las palabras reales del contexto dadas las palabras centrales.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. ¿Cómo se originan las probabilidades en Skip-gram?**\n",
    "\n",
    "Desglosemos este proceso con un ejemplo.\n",
    "\n",
    "---\n",
    "\n",
    "#### Ejemplo: **\"El gato persigue al ratón\"**\n",
    "\n",
    "Queremos predecir el contexto $ \\{\\text{\"El\"}, \\text{\"gato\"}, \\text{\"al\"}, \\text{\"ratón\"}\\} $ dado $ w_t = \\text{\"persigue\"} $.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 1: Representación One-Hot de la Palabra Central\n",
    "\n",
    "Asignamos índices al vocabulario:\n",
    "$$\n",
    "\\{\\text{\"El\"} \\to 0, \\text{\"gato\"} \\to 1, \\text{\"persigue\"} \\to 2, \\text{\"al\"} \\to 3, \\text{\"ratón\"} \\to 4\\}.\n",
    "$$\n",
    "\n",
    "La palabra central $ w_t = \\text{\"persigue\"} $ se representa como un vector one-hot:\n",
    "$$\n",
    "\\mathbf{x}_{w_t} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\  % \"El\"\n",
    "0 \\\\  % \"gato\"\n",
    "1 \\\\  % \"persigue\"\n",
    "0 \\\\  % \"al\"\n",
    "0     % \"ratón\"\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{5}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 2: Transformación a Embedding Denso\n",
    "\n",
    "Multiplicamos $ \\mathbf{x}_{w_t} $ por la matriz de embeddings de entrada $ \\mathbf{W} \\in \\mathbb{R}^{V \\times d} $ para obtener el embedding denso de la palabra central:\n",
    "$$\n",
    "\\mathbf{v}_{w_t} = \\mathbf{x}_{w_t}^\\top \\mathbf{W}.\n",
    "$$\n",
    "\n",
    "Supongamos que $ d = 2 $ y:\n",
    "$$\n",
    "\\mathbf{W} =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 \\\\  % \"El\"\n",
    "0.3 & 0.4 \\\\  % \"gato\"\n",
    "0.5 & 0.6 \\\\  % \"persigue\"\n",
    "0.7 & 0.8 \\\\  % \"al\"\n",
    "0.9 & 1.0     % \"ratón\"\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "La multiplicación selecciona la fila correspondiente a $ w_t = \\text{\"persigue\"} $:\n",
    "$$\n",
    "\\mathbf{v}_{w_t} =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.6\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 3: Proyección al Espacio del Vocabulario\n",
    "\n",
    "Para cada palabra del contexto $ w_{t+j} $, calculamos las puntuaciones $ z_{w_{t+j}} $ mediante la proyección:\n",
    "$$\n",
    "z_i = \\mathbf{u}_i^\\top \\mathbf{v}_{w_t},\n",
    "$$\n",
    "donde $ \\mathbf{u}_i $ es el vector de salida de la palabra $ i $, tomado de la matriz $ \\mathbf{U} \\in \\mathbb{R}^{V \\times d} $.\n",
    "\n",
    "Supongamos:\n",
    "$$\n",
    "\\mathbf{U} =\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.3 \\\\  % \"El\"\n",
    "0.4 & 0.5 \\\\  % \"gato\"\n",
    "0.6 & 0.7 \\\\  % \"persigue\"\n",
    "0.8 & 0.9 \\\\  % \"al\"\n",
    "1.0 & 1.1     % \"ratón\"\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Calculamos las puntuaciones para todas las palabras:\n",
    "$$\n",
    "\\mathbf{z} = \\mathbf{U} \\mathbf{v}_{w_t}.\n",
    "$$\n",
    "\n",
    "Realizando la multiplicación:\n",
    "$$\n",
    "\\mathbf{z} =\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.3 \\\\ \n",
    "0.4 & 0.5 \\\\ \n",
    "0.6 & 0.7 \\\\ \n",
    "0.8 & 0.9 \\\\ \n",
    "1.0 & 1.1\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\ \n",
    "0.6\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "1. Para \"El\":\n",
    "   $$\n",
    "   z_{\\text{\"El\"}} = 0.2 \\cdot 0.5 + 0.3 \\cdot 0.6 = 0.1 + 0.18 = 0.28.\n",
    "   $$\n",
    "2. Para \"gato\":\n",
    "   $$\n",
    "   z_{\\text{\"gato\"}} = 0.4 \\cdot 0.5 + 0.5 \\cdot 0.6 = 0.2 + 0.3 = 0.49.\n",
    "   $$\n",
    "3. Repetimos para todas las palabras:\n",
    "   $$\n",
    "   \\mathbf{z} =\n",
    "   \\begin{bmatrix}\n",
    "   0.28 \\\\  % \"El\"\n",
    "   0.49 \\\\  % \"gato\"\n",
    "   0.70 \\\\  % \"persigue\"\n",
    "   0.91 \\\\  % \"al\"\n",
    "   1.12     % \"ratón\"\n",
    "   \\end{bmatrix}.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 4: Conversión a Probabilidades (Softmax)\n",
    "\n",
    "La función softmax normaliza estas puntuaciones:\n",
    "$$\n",
    "P(w_{t+j} | w_t) = \\frac{\\exp(z_{w_{t+j}})}{\\sum_{i=1}^V \\exp(z_i)}.\n",
    "$$\n",
    "\n",
    "1. Calculamos $ \\exp(z_i) $:\n",
    "   $$\n",
    "   \\exp(\\mathbf{z}) =\n",
    "   \\begin{bmatrix}\n",
    "   1.32 \\\\ \n",
    "   1.63 \\\\ \n",
    "   2.01 \\\\ \n",
    "   2.48 \\\\ \n",
    "   3.07\n",
    "   \\end{bmatrix}.\n",
    "   $$\n",
    "\n",
    "2. Calculamos la suma:\n",
    "   $$\n",
    "   \\sum_{i=1}^V \\exp(z_i) = 1.32 + 1.63 + 2.01 + 2.48 + 3.07 = 10.51.\n",
    "   $$\n",
    "\n",
    "3. Dividimos cada $ \\exp(z_i) $ entre la suma para obtener probabilidades:\n",
    "   $$\n",
    "   P(w_{t+j} = \\text{\"El\"} | w_t) = \\frac{1.32}{10.51} \\approx 0.126,\n",
    "   $$\n",
    "   $$\n",
    "   P(w_{t+j} = \\text{\"gato\"} | w_t) = \\frac{1.63}{10.51} \\approx 0.155,\n",
    "   $$\n",
    "   y así sucesivamente.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Relación entre Probabilidades y Optimización**\n",
    "\n",
    "- Cada palabra del contexto tiene su propia probabilidad condicional $ P(w_{t+j} | w_t) $.\n",
    "- El modelo ajusta los parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento del modelo\n",
    "\n",
    "Una vez que se ha preprocesado el texto, se puede entrenar el modelo Word2Vec. Para ello, se utiliza la clase `Word2Vec` de la librería Gensim. Se pueden configurar varios parámetros, como el tamaño del vector, la ventana de contexto, el número de iteraciones, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Entrenamiento de modelo Word2Vec ####################\n",
    "\n",
    "modelo = Word2Vec(corpus_procesado, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "### Aqui cada parametro significa lo siguiente:\n",
    "# vector_size: Dimension de los vectores de palabras\n",
    "# window: Numero de palabras que se toman en cuenta para predecir la siguiente palabra\n",
    "# min_count: Frecuencia minima de palabras para ser considerada\n",
    "# sg: 0 para CBOW y 1 para Skip-gram\n",
    "\n",
    "\n",
    "\n",
    "print(modelo.wv.key_to_index)\n",
    "print(modelo.wv[\"hielo\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploración de las representaciones vectoriales\n",
    "\n",
    "Una vez entrenado el modelo, se pueden explorar las representaciones vectoriales de las palabras. Por ejemplo, se pueden obtener las palabras más similares a una palabra dada, o realizar operaciones de álgebra de vectores para encontrar relaciones entre palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Explorar representaciones vectoriales de palabras ####\n",
    "\n",
    "print(modelo.wv.most_similar(\"hielo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelo.wv.most_similar(\"padre\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALgebra de palabras ###\n",
    "\n",
    "print(modelo.wv.most_similar(positive=[\"hielo\", \"padre\"], negative=[\"perezoso\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusiones\n",
    "\n",
    "En este notebook, hemos visto cómo entrenar un modelo Word2Vec utilizando la librería Gensim y explorar las representaciones vectoriales de palabras obtenidas. Estas representaciones son útiles para realizar tareas de procesamiento de lenguaje natural, como la clasificación de texto, la traducción automática, la generación de texto, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_ean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
