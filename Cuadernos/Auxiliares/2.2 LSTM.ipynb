{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Explicación de Long Short-Term Memory (LSTM)\n","\n","En este cuaderno, explicaremos cómo funcionan las LSTM, una variante mejorada de las Redes Neuronales Recurrentes (RNN) que aborda problemas como el desvanecimiento de gradientes y las dependencias a largo plazo.\n","\n","## Objetivos\n","- Comprender el funcionamiento de las LSTM y su arquitectura interna (celdas de memoria, puertas).\n","- Explorar cómo las LSTM procesan secuencias de datos, especialmente en tareas de texto.\n","- Discutir cómo las LSTM mejoran las limitaciones de las RNN estándar.\n","- Implementar un modelo LSTM en TensorFlow para predecir la siguiente palabra en una secuencia."]},{"cell_type":"markdown","metadata":{},"source":["## 1. Introducción a las LSTM\n","Las Long Short-Term Memory (LSTM) son un tipo de RNN diseñadas para manejar dependencias a largo plazo de manera más efectiva que las RNN estándar.\n","\n","### Arquitectura de las LSTM\n","Las LSTM incluyen tres puertas clave: \n","- **Puerta de Olvido**: Decide cuánta información del estado anterior debe ser descartada.\n","- **Puerta de Entrada**: Decide cuánta información nueva debe ser almacenada en la celda de memoria.\n","- **Puerta de Salida**: Decide qué parte del estado interno debe ser salida como información al siguiente paso.\n","\n","![Arquitectura LSTM](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/1200px-The_LSTM_cell.png)\n","\n","Este diseño permite que las LSTM conserven información relevante durante periodos más largos en secuencias, mitigando el problema del desvanecimiento de gradientes presente en las RNN simples."]},{"cell_type":"markdown","metadata":{},"source":["## 2. Procesamiento de Secuencias con LSTM\n","Ahora implementaremos un modelo LSTM para procesar secuencias de texto y predecir la siguiente palabra en una oración.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Embedding\n","import numpy as np\n","\n","# Datos de ejemplo: Secuencia de texto\n","sentences = [\"el gato juega\", \"el perro corre\", \"la luna brilla\"]\n","tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","sequences = tokenizer.texts_to_sequences(sentences)\n","max_sequence_len = max([len(x) for x in sequences])\n","sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_sequence_len)\n","\n","# Dividir en características (X) y etiquetas (y)\n","X, y = sequences[:, :-1], sequences[:, -1]\n","y = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index)+1)\n","\n","# Crear el modelo LSTM\n","model_lstm = Sequential([\n","    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_sequence_len-1),\n","    LSTM(64),\n","    Dense(len(tokenizer.word_index)+1, activation='softmax')\n","])\n","\n","# Compilar el modelo\n","model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Resumen del modelo\n","model_lstm.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### Entrenamiento del Modelo LSTM\n","Entrenamos el modelo LSTM con las secuencias de texto anteriores para predecir la siguiente palabra."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Entrenar el modelo LSTM\n","history_lstm = model_lstm.fit(X, y, epochs=300, verbose=1)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Visualización del rendimiento del modelo LSTM\n","Mostramos las curvas de pérdida y precisión durante el entrenamiento del modelo."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","# Graficar la precisión y la pérdida\n","plt.plot(history_lstm.history['accuracy'], label='Precisión')\n","plt.plot(history_lstm.history['loss'], label='Pérdida')\n","plt.xlabel('Épocas')\n","plt.ylabel('Valor')\n","plt.title('Rendimiento del modelo LSTM')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Comparación entre LSTM y RNN\n","Las LSTM tienen ventajas significativas sobre las RNN estándar cuando se trata de manejar dependencias a largo plazo en las secuencias. La arquitectura interna de las LSTM les permite recordar información durante más tiempo, lo que es crucial en muchas tareas de procesamiento de lenguaje natural (NLP).\n","\n","En comparación, las RNN simples tienden a olvidar la información después de pocos pasos, lo que afecta su capacidad para manejar secuencias largas de manera efectiva."]},{"cell_type":"markdown","metadata":{},"source":["## 4. Generación de Texto con el Modelo LSTM\n","Después de entrenar el modelo, podemos usarlo para generar nuevas palabras basadas en un texto inicial."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Función para generar texto basado en el modelo LSTM entrenado\n","def generar_texto_lstm(model, tokenizer, seed_text, max_sequence_len, n_words):\n","    for _ in range(n_words):\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","        token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","        predicted = np.argmax(model.predict(token_list), axis=-1)\n","        output_word = \"\"\n","        for word, index in tokenizer.word_index.items():\n","            if index == predicted:\n","                output_word = word\n","                break\n","        seed_text += \" \" + output_word\n","    return seed_text\n","\n","# Generar texto basado en un texto inicial\n","seed_text = \"el gato\"\n","generated_text = generar_texto_lstm(model_lstm, tokenizer, seed_text, max_sequence_len, 5)\n","print(generated_text)"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusión\n","Las LSTM son una poderosa herramienta para manejar dependencias a largo plazo en secuencias de datos. Son especialmente útiles en tareas de procesamiento de lenguaje natural donde es crucial recordar el contexto de palabras anteriores. En este cuaderno, hemos implementado un modelo LSTM para predecir palabras y generar texto basado en las secuencias entrenadas."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":5}
