{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Funcionamiento de BERT y GPT\n\n", "En los \u00faltimos a\u00f1os, dos modelos han revolucionado el campo del Procesamiento del Lenguaje Natural (NLP): BERT (Bidirectional Encoder Representations from Transformers) y GPT (Generative Pre-trained Transformer). Estos modelos, basados en la arquitectura Transformer, han demostrado una capacidad sin precedentes para comprender y generar texto, superando significativamente a las t\u00e9cnicas anteriores.\n\n", "## 3.2 Introducci\u00f3n a BERT y GPT\n\n", "Mientras que BERT se destaca en tareas de comprensi\u00f3n del lenguaje, GPT sobresale en la generaci\u00f3n de texto, demostrando la versatilidad y el poder de la arquitectura Transformer en el procesamiento del lenguaje natural.\n\n", "### a. BERT (Bidirectional Encoder Representations from Transformers)\n\n", "BERT es un modelo bidireccional, lo que significa que considera el contexto de una palabra tanto a su izquierda como a su derecha. Se entrena utilizando dos tareas principales:\n\n", "- **Modelado de Lenguaje M\u00e1scara (MLM):** Se enmascaran algunas palabras en una oraci\u00f3n y el modelo intenta predecirlas.\n", "- **Predicci\u00f3n de la Pr\u00f3xima Oraci\u00f3n (NSP):** El modelo predice si una oraci\u00f3n sigue a otra en un texto.\n\n", "### b. GPT-3 (Generative Pre-trained Transformer 3)\n\n", "GPT-3 es un modelo generativo unidireccional, entrenado para predecir la pr\u00f3xima palabra en una secuencia. Utiliza una gran cantidad de datos y par\u00e1metros, lo que le permite generar texto coherente y relevante a partir de un prompt. A diferencia de BERT, que se enfoca en tareas de comprensi\u00f3n, GPT-3 se destaca en la generaci\u00f3n de texto."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ejemplo pr\u00e1ctico con BERT\n\n", "Vamos a utilizar el modelo `bert-base-uncased` para realizar una tarea de Modelado de Lenguaje M\u00e1scara (MLM)."]}, {"cell_type": "code", "metadata": {}, "source": ["from transformers import BertTokenizer, BertForMaskedLM\n", "import torch\n\n", "# Cargar el tokenizador y el modelo preentrenado BERT\n", "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n", "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n\n", "# Crear una entrada con una palabra enmascarada\n", "text = \"El modelo BERT es realmente [MASK].\"\n", "inputs = tokenizer(text, return_tensors='pt')\n\n", "# Obtener predicciones de la m\u00e1scara\n", "with torch.no_grad():\n", "    outputs = model(**inputs)\n", "    predictions = outputs.logits\n\n", "# Obtener la palabra predicha en la posici\u00f3n enmascarada\n", "masked_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n", "predicted_token_id = predictions[0, masked_index].argmax(axis=-1)\n", "predicted_token = tokenizer.decode(predicted_token_id)\n\n", "print(f'Palabra predicha: {predicted_token}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ejemplo pr\u00e1ctico con GPT-3\n\n", "GPT-3 no est\u00e1 disponible de manera gratuita para su uso directo en modelos abiertos como BERT. Sin embargo, podemos hacer una simulaci\u00f3n utilizando GPT-2, que sigue un enfoque similar.\n\n", "Vamos a utilizar el modelo `gpt2` para generar texto a partir de un prompt."]}, {"cell_type": "code", "metadata": {}, "source": ["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n", "# Cargar el tokenizador y el modelo preentrenado GPT-2\n", "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n", "model = GPT2LMHeadModel.from_pretrained('gpt2')\n\n", "# Crear un prompt de entrada\n", "prompt = \"El futuro de la inteligencia artificial es\"\n", "inputs = tokenizer(prompt, return_tensors='pt')\n\n", "# Generar texto\n", "outputs = model.generate(inputs['input_ids'], max_length=50, num_return_sequences=1)\n", "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n", "print(f'Texto generado: {generated_text}')"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}}