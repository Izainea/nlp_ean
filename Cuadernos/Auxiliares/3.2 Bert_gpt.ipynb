{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMd0gU3ifHYH"
      },
      "source": [
        "# Funcionamiento de BERT y GPT\n",
        "\n",
        "En los últimos años, dos modelos han revolucionado el campo del Procesamiento del Lenguaje Natural (NLP): BERT (Bidirectional Encoder Representations from Transformers) y GPT (Generative Pre-trained Transformer). Estos modelos, basados en la arquitectura Transformer, han demostrado una capacidad sin precedentes para comprender y generar texto, superando significativamente a las técnicas anteriores.\n",
        "\n",
        "## 3.2 Introducción a BERT y GPT\n",
        "\n",
        "Mientras que BERT se destaca en tareas de comprensión del lenguaje, GPT sobresale en la generación de texto, demostrando la versatilidad y el poder de la arquitectura Transformer en el procesamiento del lenguaje natural.\n",
        "\n",
        "### a. BERT (Bidirectional Encoder Representations from Transformers)\n",
        "\n",
        "BERT es un modelo bidireccional, lo que significa que considera el contexto de una palabra tanto a su izquierda como a su derecha. Se entrena utilizando dos tareas principales:\n",
        "\n",
        "- **Modelado de Lenguaje Máscara (MLM):** Se enmascaran algunas palabras en una oración y el modelo intenta predecirlas.\n",
        "- **Predicción de la Próxima Oración (NSP):** El modelo predice si una oración sigue a otra en un texto.\n",
        "\n",
        "### b. GPT-3 (Generative Pre-trained Transformer 3)\n",
        "\n",
        "GPT-3 es un modelo generativo unidireccional, entrenado para predecir la próxima palabra en una secuencia. Utiliza una gran cantidad de datos y parámetros, lo que le permite generar texto coherente y relevante a partir de un prompt. A diferencia de BERT, que se enfoca en tareas de comprensión, GPT-3 se destaca en la generación de texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plCsPVntfHYI"
      },
      "source": [
        "## Ejemplo práctico con BERT\n",
        "\n",
        "Vamos a utilizar el modelo `bert-base-uncased` para realizar una tarea de Modelado de Lenguaje Máscara (MLM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQU-q1eHfHYJ"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "# Cargar el tokenizador y el modelo preentrenado BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Crear una entrada con una palabra enmascarada\n",
        "text = \"El modelo BERT es realmente [MASK].\"\n",
        "inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "# Obtener predicciones de la máscara\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = outputs.logits\n",
        "\n",
        "# Obtener la palabra predicha en la posición enmascarada\n",
        "masked_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "predicted_token_id = predictions[0, masked_index].argmax(axis=-1)\n",
        "predicted_token = tokenizer.decode(predicted_token_id)\n",
        "\n",
        "print(f'Palabra predicha: {predicted_token}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyHkcgvefHYJ"
      },
      "source": [
        "## Ejemplo práctico con GPT-3\n",
        "\n",
        "GPT-3 no está disponible de manera gratuita para su uso directo en modelos abiertos como BERT. Sin embargo, podemos hacer una simulación utilizando GPT-2, que sigue un enfoque similar.\n",
        "\n",
        "Vamos a utilizar el modelo `gpt2` para generar texto a partir de un prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa8RaRG3fHYJ"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Cargar el tokenizador y el modelo preentrenado GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Crear un prompt de entrada\n",
        "prompt = \"El futuro de la inteligencia artificial es\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "# Generar texto\n",
        "outputs = model.generate(inputs['input_ids'], max_length=50, num_return_sequences=1)\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f'Texto generado: {generated_text}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}