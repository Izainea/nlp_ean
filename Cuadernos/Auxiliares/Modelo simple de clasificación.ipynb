{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo no supervisado de clasificación de texto\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este notebook se presenta un modelo no supervisado de clasificación de texto. El modelo se basa en el uso de embeddings de palabras y clustering. Se utiliza el algoritmo de clustering KMeans para agrupar los textos en clusters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Librerias necesarias Doc2Vec\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "### Librerias necesarias para el preprocesamiento de texto\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "### Descargar stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "### Otras librerias necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar el texto\n",
    "\n",
    "El dataset \"es_tweets_laboral\" de la colección \"somosnlp-hackathon-2022\" en Hugging Face está diseñado específicamente para el análisis de texto relacionado con temas laborales en español. Este dataset contiene tuits que abordan temas laborales, y es ideal para tareas de clasificación de texto, análisis de sentimientos, y otras aplicaciones de procesamiento de lenguaje natural (NLP) enfocadas en el ámbito laboral.\n",
    "\n",
    "**Características del Dataset:**\n",
    "- *Contenido*: Incluye tuits en español relacionados con temas laborales, como empleo, condiciones de trabajo, y derechos laborales.\n",
    "- *Etiquetas*: Los tuits pueden estar etiquetados según el tema o el sentimiento, lo que facilita su uso en tareas de clasificación supervisada.\n",
    "- *Aplicaciones*: Este dataset es útil para construir modelos que analicen la percepción de los usuarios sobre temas laborales, detectar tendencias en el mercado laboral, o identificar problemas comunes en el ámbito laboral.\n",
    "\n",
    "El dataset es parte de un esfuerzo colaborativo durante el Hackathon de SomosNLP en 2022, que busca fomentar el desarrollo de tecnologías de procesamiento de lenguaje natural en español.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Carga dataset desde huggingface\n",
    "\n",
    "from datasets import load_dataset\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# Cargando el dataset \"es_tweets_laboral\" desde Hugging Face\n",
    "dataset = load_dataset(\"somosnlp-hackathon-2022/es_tweets_laboral\")\n",
    "\n",
    "# Explorando el contenido del dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "test = dataset['test']\n",
    "\n",
    "# Explorando el contenido de los datos de entrenamiento\n",
    "\n",
    "train_df = train.to_pandas()\n",
    "test_df = test.to_pandas()\n",
    "print(\"Shape of train data: \", train_df.shape)\n",
    "print(\"Shape of test data: \", test_df.shape)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento\n",
    "\n",
    "Empezamos por cargar el dataset y realizar un preprocesamiento básico de los textos. En este caso, se eliminan las menciones a usuarios, los enlaces, y los caracteres especiales. Además, se convierten los textos a minúsculas y se eliminan las stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Preprocesamiento de texto ###############\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Eliminando caracteres especiales y números\n",
    "    text = re.sub(r'[^a-zA-ZáéíóúÁÉÍÓÚ\\s]', '', text, re.I|re.A)\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # eliminando stopwords\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    text = [i for i in word_tokens if not i in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "# Aplicando la función de preprocesamiento a los datos de entrenamiento y prueba\n",
    "\n",
    "train_df['text_pre'] = train_df['text'].apply(preprocess_text)\n",
    "\n",
    "test_df['text_pre'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelo de Clasificación de Texto\n",
    "\n",
    "Una vez que hemos preprocesado los textos, podemos aplicar un modelo de clasificación no supervisado para agruparlos en categorías o clusters. En este caso, utilizaremos el algoritmo de clustering KMeans para agrupar los textos en clusters. AUnque primero debemos convertir los textos en vectores numéricos utilizando embeddings de palabras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Entrenamiento de Word2Vec ####################\n",
    "\n",
    "# Tokenizando el texto\n",
    "\n",
    "train_df['text_tokens'] = train_df['text_pre'].apply(lambda x: x.split())\n",
    "test_df['text_tokens'] = test_df['text_pre'].apply(lambda x: x.split())\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenando el modelo Doc2Vec\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_df['text_tokens'])]\n",
    "\n",
    "# Definiendo el modelo Doc2Vec\n",
    "\n",
    "model = Doc2Vec(documents, vector_size=100, window=2, min_count=1, workers=4)\n",
    "\n",
    "# Guardando el modelo entrenado\n",
    "\n",
    "model.save(\"doc2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargando el modelo entrenado\n",
    "\n",
    "model = Doc2Vec.load(\"doc2vec.model\")\n",
    "\n",
    "# Obteniendo el vector de una palabra\n",
    "\n",
    "model.wv['trabajo']\n",
    "\n",
    "# Obteniendo las palabras más similares a una palabra\n",
    "\n",
    "model.wv.most_similar('trabajo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo la similitud entre dos palabras\n",
    "\n",
    "model.wv.similarity('trabajo', 'empleo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hacemos un clustering de los tweets con KMeans\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Obteniendo los vectores de los tweets\n",
    "\n",
    "vectors = [model.infer_vector(doc) for doc in train_df['text_tokens']]\n",
    "\n",
    "# Definiendo el modelo KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "\n",
    "# Entrenando el modelo KMeans\n",
    "\n",
    "kmeans.fit(vectors)\n",
    "\n",
    "# Obteniendo las etiquetas de los clusters\n",
    "\n",
    "train_df['cluster'] = kmeans.labels_\n",
    "\n",
    "# Explorando los clusters\n",
    "\n",
    "train_df['cluster'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Veamos los tweets de un cluster\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "train_df[train_df['cluster'] == 0]['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Veamos el centroide de cada cluster\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Cluster \", i)\n",
    "    print(model.wv.most_similar(positive=[kmeans.cluster_centers_[i]], topn=10))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_ean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
