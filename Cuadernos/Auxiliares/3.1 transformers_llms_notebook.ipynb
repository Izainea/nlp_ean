{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Introducción a los Transformers y Modelos de Lenguaje de Gran Tamaño (LLMs)\n","\n","En esta guía, vamos a explorar los conceptos clave detrás de los Transformers, que han revolucionado el campo del Procesamiento de Lenguaje Natural (NLP). Los modelos basados en Transformers, como BERT y GPT-3, son capaces de manejar secuencias largas de texto y capturar relaciones complejas entre palabras en diferentes posiciones del texto.\n","\n","## Objetivos\n","- Comprender cómo funciona el mecanismo de atención en los Transformers.\n","- Explorar la atención multi-cabeza para captar diferentes relaciones contextuales.\n","- Entender cómo los Transformers incorporan la información de posición mediante codificaciones posicionales.\n","- Analizar la estructura de las capas de transformación y su papel en los Transformers."]},{"cell_type":"markdown","metadata":{},"source":["## 1. Atención (Attention)\n","\n","El corazón de los Transformers es el mecanismo de atención. Este mecanismo permite al modelo enfocarse en diferentes partes del texto cuando procesa una palabra, lo que es crucial para capturar dependencias a largo plazo.\n","\n","La atención se calcula utilizando las matrices de Query (Q), Key (K) y Value (V):\n","\n","$$\n","Attention(Q, K, V) = Softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n","$$\n","\n","Aquí, Q representa las consultas, K las claves y V los valores. \\( d_k \\) es la dimensión de las claves. El producto punto entre Q y K seguido de la normalización softmax da como resultado los pesos de atención, que se usan para ponderar los valores V."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Ejemplo simple de cálculo de atención en TensorFlow\n","import tensorflow as tf\n","\n","# Definir las matrices de Query (Q), Key (K) y Value (V)\n","Q = tf.random.normal(shape=(1, 5, 64))  # Query\n","K = tf.random.normal(shape=(1, 5, 64))  # Key\n","V = tf.random.normal(shape=(1, 5, 64))  # Value\n","\n","# Calcular la atención\n","dk = tf.cast(tf.shape(K)[-1], tf.float32)\n","scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(dk)\n","attention_weights = tf.nn.softmax(scores, axis=-1)\n","attention_output = tf.matmul(attention_weights, V)\n","attention_output"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Multi-Head Attention\n","\n","Los Transformers utilizan múltiples cabezas de atención para captar diferentes tipos de relaciones contextuales en el texto. Cada cabeza procesa la información de manera diferente, y los resultados se concatenan y se combinan linealmente:\n","\n","$$\n","MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h)W^O\n","$$\n","\n","Donde cada `head_i` se calcula como una instancia independiente de la atención, pero con diferentes proyecciones de Q, K y V."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Ejemplo de implementación de Multi-Head Attention en TensorFlow\n","multihead_attention = tf.keras.layers.MultiHeadAttention(num_heads=8, key_dim=64)\n","output = multihead_attention(Q, K, V)\n","output"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Codificación Posicional (Positional Encoding)\n","\n","Los Transformers no procesan las secuencias en orden secuencial como las RNNs, por lo que necesitan una manera de incorporar la información de posición. Esto se hace mediante codificaciones posicionales que se suman a los embeddings de las palabras.\n","\n","La codificación posicional utiliza las siguientes fórmulas para las dimensiones pares e impares del embedding:\n","\n","$$\n","PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n","$$\n","$$\n","PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","# Implementación de codificación posicional\n","def positional_encoding(position, d_model):\n","    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / d_model)\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Dimensiones pares\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Dimensiones impares\n","    return angle_rads\n","\n","# Generar codificación posicional para una secuencia de 50 posiciones y un embedding de tamaño 512\n","pos_encoding = positional_encoding(50, 512)\n","pos_encoding.shape"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Capas de Transformación\n","\n","Los Transformers están compuestos por capas repetitivas que incluyen subcapas de atención y redes feed-forward. Cada capa sigue la estructura:\n","\n","$$\n","LayerNorm(x + Sublayer(x))\n","$$\n","\n","Donde `Sublayer(x)` puede ser una subcapa de atención o una red feed-forward. La normalización por capas y las conexiones residuales ayudan a estabilizar el entrenamiento y a mantener la integridad del gradiente."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Ejemplo de una capa de Transformer\n","from tensorflow.keras.layers import LayerNormalization, Dense, Dropout\n","\n","# Definir una capa de transformación con atención y red feed-forward\n","class TransformerLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff):\n","        super(TransformerLayer, self).__init__()\n","        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n","        self.ffn = tf.keras.Sequential([\n","            Dense(dff, activation='relu'),\n","            Dense(d_model)\n","        ])\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = Dropout(0.1)\n","        self.dropout2 = Dropout(0.1)\n","\n","    def call(self, x, training):\n","        attn_output = self.mha(x, x, x)  # self-attention\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)  # Residual connection + normalization\n","\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","# Usar la capa de Transformer\n","sample_transformer_layer = TransformerLayer(d_model=512, num_heads=8, dff=2048)\n","x = tf.random.uniform((64, 50, 512))  # Batch de secuencias\n","out = sample_transformer_layer(x, training=False)\n","out.shape"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8"}},"nbformat":4,"nbformat_minor":2}
