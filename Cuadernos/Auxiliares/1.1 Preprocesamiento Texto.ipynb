{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de texto\n",
    "\n",
    "## Introducción\n",
    "\n",
    "El preprocesamiento de texto es una tarea elemental en el procesamiento de lenguaje natural. Consiste en una serie de pasos que se realizan antes de aplicar cualquier algoritmo de aprendizaje automático, si bien en algunas nuevas herramientas de procesamiento de lenguaje natural, como BERT, se realizan algunos pasos de preprocesamiento de forma implícita, es importante conocer los pasos que se realizan en dicho proceso.En este notebook, se presentan los pasos más comunes. \n",
    "\n",
    "## 1. Importar librerías\n",
    "\n",
    "En este caso, se importarán las librerías necesarias para realizar el preprocesamiento de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re ## Exprexiones regulares\n",
    "import nltk ## Procesamiento de lenguaje natural\n",
    "from nltk.corpus import stopwords ## Palabras vacias\n",
    "from nltk.stem import SnowballStemmer ## Stemming\n",
    "from nltk.tokenize import word_tokenize ## Tokenizacion\n",
    "from nltk.tokenize import RegexpTokenizer ## Tokenizacion\n",
    "from sklearn.feature_extraction.text import CountVectorizer ## Vectorizador\n",
    "\n",
    "\n",
    "############## Descarga de recursos de nltk ################\n",
    "nltk.download('punkt') ## Tokenizador\n",
    "nltk.download('stopwords') ## Palabras vacias\n",
    "nltk.download('snowball_data') ## Stemming\n",
    "nltk.download('wordnet') ## Lematizacion\n",
    "\n",
    "############## Es necesario descargar estos recursos para poder ejecutar el script ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar el texto\n",
    "\n",
    "Para este ejercicio inicial usaremos un texto sencillo, el cual se cargará en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"Hace varios años, en el pelotón de fusilamiento, el coronel Aureliano Buendía había de recordar aquella tarde remota en que su padre lo llevó a conocer el hielo. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalización\n",
    "\n",
    "La normalización es el proceso de convertir texto en un formato estándar. En este caso, se convertirá el texto a minúsculas. En este paso también se pueden eliminar caracteres especiales, números, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Normalización ################\n",
    "\n",
    "## Convertir a minusculas\n",
    "\n",
    "texto = texto.lower()\n",
    "\n",
    "## Eliminar caracteres especiales\n",
    "\n",
    "texto = re.sub(r\"[\\W_]+\", \" \", texto) ## Elimina caracteres especiales y numeros la expresion regular \\W es para caracteres especiales y \\d para numeros, todo lo que no sea una letra se reemplaza por un espacio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenización\n",
    "\n",
    "La tokenización es el proceso de dividir el texto en palabras, frases u oraciones. En este caso, se dividirá el texto en palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Tokenizacion ################\n",
    "\n",
    "## Tokenizador por palabras\n",
    "\n",
    "texto_tokenizado = word_tokenize(texto)\n",
    "\n",
    "### La funcion word_tokenize de nltk permite tokenizar un texto en palabras, por detrás esta utilizando expresiones regulares para separar las palabras.\n",
    "\n",
    "texto_tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizador por expresiones regulares\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "texto_tokenizado = tokenizer.tokenize(texto)\n",
    "\n",
    "### La clase RegexpTokenizer de nltk permite tokenizar un texto utilizando expresiones regulares, en este caso se tokeniza por palabras.\n",
    "\n",
    "texto_tokenizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Eliminación de stopwords\n",
    "\n",
    "Las stopwords son palabras que no aportan significado al texto, como artículos, preposiciones, etc. En este paso, se eliminarán las stopwords del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Eliminación de palabras vacias ################\n",
    "\n",
    "## Palabras vacias en español\n",
    "\n",
    "stopwords_esp = stopwords.words('spanish')\n",
    "\n",
    "## Eliminar palabras vacias\n",
    "\n",
    "texto_filtrado = [palabra for palabra in texto_tokenizado if palabra not in stopwords_esp]\n",
    "\n",
    "### La lista stopwords.words('spanish') contiene las palabras vacias en español, se filtran las palabras vacias del texto tokenizado.\n",
    "\n",
    "stopwords_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lematización o stemming\n",
    "\n",
    "La lematización es el proceso de convertir una palabra a su forma base. Por ejemplo, las palabras \"corriendo\" y \"corrió\" se convertirían a \"correr\". En este paso, se lematizarán las palabras del texto. No obstante, también se puede realizar stemming, que es un proceso similar, pero menos preciso, en el cual se eliminan sufijos y prefijos de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Stemming ################\n",
    "\n",
    "## Stemmer en español\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "## Stemming\n",
    "\n",
    "texto_stemming = [stemmer.stem(palabra) for palabra in texto_filtrado]\n",
    "\n",
    "### La clase SnowballStemmer de nltk permite realizar stemming en español, se aplica stemming a las palabras filtradas.\n",
    "\n",
    "texto_stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Lematización ################\n",
    "\n",
    "## Lematizador en español\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "## Lematización\n",
    "\n",
    "texto_lematizado = [lemmatizer.lemmatize(palabra) for palabra in texto_filtrado]\n",
    "\n",
    "### La clase WordNetLemmatizer de nltk permite realizar lematización en inglés, se aplica lematización a las palabras filtradas.\n",
    "\n",
    "texto_lematizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bag of words\n",
    "\n",
    "Finalmente, se creará una representación de las palabras del texto en forma de vector. En este caso, se usará la técnica de bag of words, que consiste en contar la frecuencia de las palabras en el texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Bag of words ################\n",
    "\n",
    "## Vectorizador\n",
    "\n",
    "vectorizador = CountVectorizer()\n",
    "\n",
    "## Bolsa de palabras\n",
    "\n",
    "texto_bow = vectorizador.fit_transform([' '.join(texto_lematizado)])\n",
    "\n",
    "## Veamos un dataframe con la bolsa de palabras\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(texto_bow.toarray(), columns=vectorizador.get_feature_names_out())\n",
    "df_index = pd.DataFrame([' '.join(texto_lematizado)], columns=['Texto'])\n",
    "\n",
    "df = pd.concat([df_index, df], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusiones\n",
    "\n",
    "En este notebook, se presentaron los pasos más comunes en el preprocesamiento de texto. Es importante tener en cuenta que estos pasos pueden variar dependiendo del problema y del texto que se esté analizando. Para las siguientes tareas es probable que se hagan algunos de los pasos mencionados anteriormente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_ean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
